{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5vk0lh6_nvx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "  @staticmethod\n",
        "  def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test"
      ],
      "metadata": {
        "id": "UPsghHfW_3oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "  @staticmethod\n",
        "  def preprocess_data(x_train, x_val, x_test):\n",
        "    x_train = x_train.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
        "    x_val = x_val.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
        "    return x_train, x_val, x_test"
      ],
      "metadata": {
        "id": "swdgphooB2sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelBuilder:\n",
        "  @staticmethod\n",
        "  def build_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Entry Flow\n",
        "    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), activation='relu', input_shape=input_shape, padding='same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.DepthwiseConv2D((3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(64, (1, 1), activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Conv2D(128, (1, 1), activation='relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QvBm1LXFCZwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "  def __init__(self, model, optimizer, loss_function, metrics):\n",
        "    self.model = model\n",
        "    self.model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
        "\n",
        "  def train_model(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
        "    history = self.model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val), verbose=2)\n",
        "    return history"
      ],
      "metadata": {
        "id": "4uwL56Sd7zQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  x_train, y_train, x_val, y_val, x_test, y_test = DataLoader.load_data()\n",
        "  x_train, x_val, x_test = DataPreprocessor.preprocess_data(x_train, x_val, x_test)\n",
        "\n",
        "  input_shape = x_train.shape[1:]\n",
        "  num_classes = 10\n",
        "\n",
        "  model = ModelBuilder.build_model(input_shape, num_classes)\n",
        "\n",
        "  optimizer ='adam'\n",
        "  loss_function = 'sparse_categorical_crossentropy'\n",
        "  metrics = ['accuracy']\n",
        "\n",
        "  trainer = Trainer(model, optimizer, loss_function, metrics)\n",
        "\n",
        "  epochs = 5\n",
        "  batch_size = 32\n",
        "\n",
        "  history = trainer.train_model(x_train, y_train, x_val, y_val, epochs, batch_size)\n",
        "\n",
        "  test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=2)\n",
        "  print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "cOlD2nfY8h_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MbyCNiH9rRO",
        "outputId": "a8f7db5c-2de3-4307-965e-9bfe8e133965"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1500/1500 - 31s - loss: 0.4680 - accuracy: 0.8779 - val_loss: 0.4086 - val_accuracy: 0.8714 - 31s/epoch - 21ms/step\n",
            "Epoch 2/5\n",
            "1500/1500 - 30s - loss: 0.1389 - accuracy: 0.9599 - val_loss: 0.2137 - val_accuracy: 0.9355 - 30s/epoch - 20ms/step\n",
            "Epoch 3/5\n",
            "1500/1500 - 29s - loss: 0.1096 - accuracy: 0.9672 - val_loss: 0.1260 - val_accuracy: 0.9631 - 29s/epoch - 19ms/step\n",
            "Epoch 4/5\n",
            "1500/1500 - 29s - loss: 0.0905 - accuracy: 0.9730 - val_loss: 0.1694 - val_accuracy: 0.9505 - 29s/epoch - 19ms/step\n",
            "Epoch 5/5\n",
            "1500/1500 - 29s - loss: 0.0810 - accuracy: 0.9750 - val_loss: 0.2264 - val_accuracy: 0.9297 - 29s/epoch - 20ms/step\n",
            "313/313 - 2s - loss: 0.2308 - accuracy: 0.9292 - 2s/epoch - 6ms/step\n",
            "Test Accuracy: 92.92%\n"
          ]
        }
      ]
    }
  ]
}